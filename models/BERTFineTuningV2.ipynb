{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Pip Installs I needed to Perform"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T11:25:58.992615Z","iopub.status.busy":"2024-10-17T11:25:58.992227Z","iopub.status.idle":"2024-10-17T11:25:58.999060Z","shell.execute_reply":"2024-10-17T11:25:58.998079Z","shell.execute_reply.started":"2024-10-17T11:25:58.992571Z"},"trusted":true},"outputs":[],"source":["#!pip install transformers\n","#!pip install torch"]},{"cell_type":"markdown","metadata":{},"source":["# Imports"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-10-17T16:13:56.119725Z","iopub.status.busy":"2024-10-17T16:13:56.119264Z","iopub.status.idle":"2024-10-17T16:14:09.300332Z","shell.execute_reply":"2024-10-17T16:14:09.299440Z","shell.execute_reply.started":"2024-10-17T16:13:56.119682Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import re\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","import torch\n","from transformers import DistilBertTokenizer\n","from sklearn.preprocessing import LabelEncoder"]},{"cell_type":"markdown","metadata":{},"source":["# Training data reading and preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T16:14:10.113855Z","iopub.status.busy":"2024-10-17T16:14:10.112830Z","iopub.status.idle":"2024-10-17T16:14:10.667407Z","shell.execute_reply":"2024-10-17T16:14:10.666254Z","shell.execute_reply.started":"2024-10-17T16:14:10.113809Z"},"trusted":true},"outputs":[],"source":["data = pd.read_csv('/kaggle/input/train-set/train.txt', sep='\\t', header=None, names=['Title', 'Origin', 'Genre', 'Director', 'Plot'])\n","print(\"Genre value counts\")\n","print(data['Genre'].value_counts())"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T16:14:12.743063Z","iopub.status.busy":"2024-10-17T16:14:12.742173Z","iopub.status.idle":"2024-10-17T16:14:14.192161Z","shell.execute_reply":"2024-10-17T16:14:14.191168Z","shell.execute_reply.started":"2024-10-17T16:14:12.743020Z"},"trusted":true},"outputs":[],"source":["def preprocess_text(text):\n","    text = text.strip()\n","    text = re.sub('\\s+', ' ', text)\n","    return text\n","\n","data['Clean_Plot'] = data['Plot'].apply(preprocess_text)"]},{"cell_type":"markdown","metadata":{},"source":["# BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T16:14:14.195309Z","iopub.status.busy":"2024-10-17T16:14:14.194672Z","iopub.status.idle":"2024-10-17T16:14:14.237893Z","shell.execute_reply":"2024-10-17T16:14:14.236478Z","shell.execute_reply.started":"2024-10-17T16:14:14.195261Z"},"trusted":true},"outputs":[],"source":["# make sure the output is \"Using device: cuda\" for your own sake!!!\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","print(f\"Using device: {device}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Encode and train-test split"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T16:14:15.606450Z","iopub.status.busy":"2024-10-17T16:14:15.606069Z","iopub.status.idle":"2024-10-17T16:14:15.631867Z","shell.execute_reply":"2024-10-17T16:14:15.630865Z","shell.execute_reply.started":"2024-10-17T16:14:15.606413Z"},"trusted":true},"outputs":[],"source":["encoder = LabelEncoder()\n","data['GenreEncoded'] = encoder.fit_transform(data['Genre'])\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    data['Clean_Plot'], data['GenreEncoded'], \n","    test_size=0.2, \n","    random_state=42, \n","    stratify=data['GenreEncoded']\n",")\n","X_train = X_train.reset_index(drop=True)\n","y_train = y_train.reset_index(drop=True)\n","X_test = X_test.reset_index(drop=True)\n","y_test = y_test.reset_index(drop=True)\n","print(\"done\")"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T11:26:14.963035Z","iopub.status.busy":"2024-10-17T11:26:14.962680Z","iopub.status.idle":"2024-10-17T11:26:14.967357Z","shell.execute_reply":"2024-10-17T11:26:14.966545Z","shell.execute_reply.started":"2024-10-17T11:26:14.963000Z"},"trusted":true},"outputs":[],"source":["# # Custom Dataset altered to deal with my idea for the chunking strategy, but that didn't go well -- Future Work\n","\n","# class MovieGenreDataset(torch.utils.data.Dataset):\n","#     def __init__(self, encodings, labels, plot_indices):\n","#         self.encodings = encodings\n","#         self.labels = labels\n","#         self.plot_indices = plot_indices\n","\n","#     def __getitem__(self, idx):\n","#         item = {key: val[idx] for key, val in self.encodings.items()}\n","#         # No need to add labels here if using Trainer with labels in encodings\n","#         return item\n","\n","#     def __len__(self):\n","#         return len(self.encodings['input_ids'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T16:14:17.719400Z","iopub.status.busy":"2024-10-17T16:14:17.718965Z"},"trusted":true},"outputs":[],"source":["# ## OLD VERSION -- simply truncates plots with more than 512 tokens, leaving them with 512 tokens\n","\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","\n","def tokenize_data(texts, max_length=512):\n","    return tokenizer(\n","       texts.tolist(),\n","       padding=True,\n","       truncation=True,\n","       max_length=max_length,\n","       return_tensors='pt'\n","   )\n","\n","train_encodings = tokenize_data(X_train)\n","test_encodings = tokenize_data(X_test)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Custom Dataset Class"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class MovieGenreDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: val[idx] for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = MovieGenreDataset(train_encodings, y_train)\n","test_dataset = MovieGenreDataset(test_encodings, y_test)"]},{"cell_type":"markdown","metadata":{},"source":["## Useful to see how many plots exceed 512 tokens"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T11:28:10.827630Z","iopub.status.busy":"2024-10-17T11:28:10.827338Z","iopub.status.idle":"2024-10-17T11:28:10.838775Z","shell.execute_reply":"2024-10-17T11:28:10.837955Z","shell.execute_reply.started":"2024-10-17T11:28:10.827598Z"},"trusted":true},"outputs":[],"source":["# ## TODO remove at the end (?)\n","\n","# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","\n","# def count_tokens(texts, tokenizer):\n","#     token_counts = []\n","#     for text in texts:\n","#         tokens = tokenizer.encode(text, truncation=False)  # No truncation to check full token count\n","#         token_counts.append(len(tokens))\n","#     return token_counts\n","\n","# token_counts = count_tokens(data['Clean_Plot'].tolist(), tokenizer)\n","# over_512_count = sum([1 for count in token_counts if count > 512])\n","\n","# print(f\"Number of plots exceeding 512 tokens: {over_512_count}\")"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T11:28:10.840185Z","iopub.status.busy":"2024-10-17T11:28:10.839870Z","iopub.status.idle":"2024-10-17T11:28:10.854647Z","shell.execute_reply":"2024-10-17T11:28:10.853740Z","shell.execute_reply.started":"2024-10-17T11:28:10.840153Z"},"trusted":true},"outputs":[],"source":["# # NEW VERSION OF ENCODINGS THAT MERGE CHUNKS\n","\n","# tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n","\n","# def chunk_text(text, max_length=512):\n","#     words = text.split()\n","#     return [' '.join(words[i:i + max_length]) for i in range(0, len(words), max_length)]\n","\n","# def tokenize_chunked_data(texts, labels, max_length=512):\n","#     all_chunks = []\n","#     all_labels = []\n","#     for i, text in enumerate(texts):\n","#         chunks = chunk_text(text, max_length)\n","#         all_chunks.extend(chunks)\n","#         if labels is not None:\n","#             all_labels.extend([labels[i]] * len(chunks))\n","#     encodings = tokenizer(all_chunks, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n","#     return encodings, all_labels\n","\n","# train_encodings, train_labels = tokenize_chunked_data(X_train, y_train)\n","# test_encodings, test_labels = tokenize_chunked_data(X_test, y_test)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T11:28:10.856013Z","iopub.status.busy":"2024-10-17T11:28:10.855634Z","iopub.status.idle":"2024-10-17T11:28:10.866726Z","shell.execute_reply":"2024-10-17T11:28:10.865815Z","shell.execute_reply.started":"2024-10-17T11:28:10.855980Z"},"trusted":true},"outputs":[],"source":["# Experimental version where I tried to actually implement the version that I had envisioned but this didn't go well at all -- Future Work\n","\n","# def chunk_text(text, tokenizer, max_length=512, stride=0):\n","#     tokens = tokenizer.encode(text, add_special_tokens=False)\n","#     # Split tokens into chunks with optional stride\n","#     chunks = []\n","#     for i in range(0, len(tokens), max_length - stride):\n","#         chunk = tokens[i:i + max_length]\n","#         chunks.append(chunk)\n","#     return chunks\n","\n","\n","# def tokenize_chunked_data(texts, labels, tokenizer, max_length=512, stride=0):\n","#     all_input_ids = []\n","#     all_attention_masks = []\n","#     all_labels = []\n","#     for i, text in enumerate(texts):\n","#         chunks = chunk_text(text, tokenizer, max_length=max_length, stride=stride)\n","#         for chunk in chunks:\n","#             # Prepare inputs for the model\n","#             inputs = tokenizer.prepare_for_model(\n","#                 chunk,\n","#                 max_length=max_length,\n","#                 padding='max_length',\n","#                 truncation=True,\n","#                 return_tensors='pt'\n","#             )\n","#             all_input_ids.append(inputs['input_ids'])\n","#             all_attention_masks.append(inputs['attention_mask'])\n","#             if labels is not None:\n","#                 all_labels.append(labels[i])\n","#     # torch.cat concatenates tensors\n","#     all_input_ids = torch.cat(all_input_ids, dim=0)\n","#     all_attention_masks = torch.cat(all_attention_masks, dim=0)\n","#     return {'input_ids': all_input_ids, 'attention_mask': all_attention_masks}, all_labels\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T11:28:10.868185Z","iopub.status.busy":"2024-10-17T11:28:10.867867Z","iopub.status.idle":"2024-10-17T11:28:10.880736Z","shell.execute_reply":"2024-10-17T11:28:10.879872Z","shell.execute_reply.started":"2024-10-17T11:28:10.868153Z"},"trusted":true},"outputs":[],"source":["# print(len(train_encodings['input_ids']), len(train_labels))\n","# print(len(test_encodings['input_ids']), len(test_labels))\n","\n","# assert len(train_encodings['input_ids']) == len(train_labels), \"Mismatch between encodings and labels\"\n","# assert len(test_encodings['input_ids']) == len(test_labels), \"Mismatch between encodings and labels\""]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T11:28:10.882230Z","iopub.status.busy":"2024-10-17T11:28:10.881970Z","iopub.status.idle":"2024-10-17T11:28:10.891341Z","shell.execute_reply":"2024-10-17T11:28:10.890587Z","shell.execute_reply.started":"2024-10-17T11:28:10.882201Z"},"trusted":true},"outputs":[],"source":["# input_ids = train_encodings['input_ids'][0].tolist()\n","# tokens = tokenizer.convert_ids_to_tokens(input_ids)\n","# print(tokens)"]},{"cell_type":"markdown","metadata":{},"source":["## Import Pre-Trained Model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import DistilBertForSequenceClassification\n","model = (DistilBertForSequenceClassification\n","        .from_pretrained('distilbert-base-uncased', num_labels=len(encoder.classes_))\n","        .to(device))"]},{"cell_type":"markdown","metadata":{},"source":["## Custom Metrics function"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T11:28:13.119500Z","iopub.status.busy":"2024-10-17T11:28:13.118942Z","iopub.status.idle":"2024-10-17T11:28:13.125349Z","shell.execute_reply":"2024-10-17T11:28:13.124466Z","shell.execute_reply.started":"2024-10-17T11:28:13.119465Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","from sklearn.metrics import accuracy_score, f1_score\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    acc = accuracy_score(labels, predictions)\n","    f1 = f1_score(labels, predictions, average='weighted')\n","    print(f\"Accuracy: {acc}, F1-score: {f1}\")\n","    return {'accuracy': acc, 'f1': f1}"]},{"cell_type":"markdown","metadata":{},"source":["## Training (Fine-Tuning)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T11:28:13.126735Z","iopub.status.busy":"2024-10-17T11:28:13.126468Z","iopub.status.idle":"2024-10-17T11:37:49.808132Z","shell.execute_reply":"2024-10-17T11:37:49.807290Z","shell.execute_reply.started":"2024-10-17T11:28:13.126704Z"},"trusted":true},"outputs":[],"source":["from transformers import Trainer, TrainingArguments\n","\n","from transformers import DataCollatorWithPadding\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","training_args = TrainingArguments(\n","    output_dir='./results',\n","    eval_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=3, # we tested with more than 3 epochs but it didn't improve the results\n","    weight_decay=0.01,               \n","    logging_steps=100,\n","    logging_dir='./logs',\n","    report_to=\"none\" # supress annoying warning\n",")\n","\n","# parameters for training (fine-tuning)\n","trainer = Trainer(\n","    model=model, # pre-trained distilBert\n","    args=training_args, # custom training args\n","    train_dataset=train_dataset, # custom datasets\n","    eval_dataset=test_dataset, \n","    compute_metrics=compute_metrics,\n","    data_collator=data_collator,\n",")\n","\n","# actual fine-tuning of the model we feed it\n","trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["## Saves the model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T11:37:49.810032Z","iopub.status.busy":"2024-10-17T11:37:49.809312Z","iopub.status.idle":"2024-10-17T11:37:50.436554Z","shell.execute_reply":"2024-10-17T11:37:50.435593Z","shell.execute_reply.started":"2024-10-17T11:37:49.809994Z"},"trusted":true},"outputs":[],"source":["model.save_pretrained('./fineTunedDistilbertWithWeirdChunking')\n","tokenizer.save_pretrained('./fineTunedDistilbertWithWeirdChunking')"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation Metrics (Accuracy, F1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T11:37:50.438658Z","iopub.status.busy":"2024-10-17T11:37:50.438224Z","iopub.status.idle":"2024-10-17T11:38:20.170962Z","shell.execute_reply":"2024-10-17T11:38:20.170057Z","shell.execute_reply.started":"2024-10-17T11:37:50.438614Z"},"trusted":true},"outputs":[],"source":["eval_result = trainer.evaluate()\n","predictions_output = trainer.predict(test_dataset)\n","predicted_labels = np.argmax(predictions_output.predictions, axis=-1)\n","true_labels = y_test.values\n","print(f\"Evaluation Results: {eval_result}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Confusion Matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T11:38:20.172630Z","iopub.status.busy":"2024-10-17T11:38:20.172231Z","iopub.status.idle":"2024-10-17T11:38:20.661336Z","shell.execute_reply":"2024-10-17T11:38:20.660449Z","shell.execute_reply.started":"2024-10-17T11:38:20.172584Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","confusion_matrix = confusion_matrix(true_labels, predicted_labels)\n","display = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=encoder.classes_)\n","display.plot(cmap=\"Blues\", values_format=\"d\")\n","plt.title(\"Confusion Matrix\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Manually Analyze Misclassifications"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T11:51:48.219478Z","iopub.status.busy":"2024-10-17T11:51:48.219086Z","iopub.status.idle":"2024-10-17T11:51:48.362920Z","shell.execute_reply":"2024-10-17T11:51:48.361911Z","shell.execute_reply.started":"2024-10-17T11:51:48.219441Z"},"trusted":true},"outputs":[],"source":["misclassified_indices = np.where(predicted_labels != true_labels)[0]\n","\n","# # displays first 10 misclassified examples to analyze in the console\n","# for idx in misclassified_indices[:10]:\n","#     print(f\"True label: {encoder.inverse_transform([true_labels[idx]])[0]}, Predicted: {encoder.inverse_transform([predicted_labels[idx]])[0]}\")\n","#     print(f\"Text: {X_test.iloc[idx]}\")\n","#     print(\"-\" * 50)\n","\n","# saves mispredictions to a file to manually analyze them for the report\n","with open('./misclassified_plots.txt', 'w') as f:\n","    f.write(\"Misclassified Plots:\\n\")\n","    f.write(\"=\"*80 + \"\\n\")\n","    for idx in misclassified_indices:\n","        true_label = encoder.inverse_transform([true_labels[idx]])[0]\n","        predicted_label = encoder.inverse_transform([predicted_labels[idx]])[0]\n","        plot_text = X_test.iloc[idx]\n","        f.write(f\"True label: {true_label}, Predicted: {predicted_label}\\n\")\n","        f.write(f\"Plot: {plot_text}\\n\")\n","        f.write(\"-\" * 80 + \"\\n\")\n","\n","print(\"Done Writing\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T11:54:05.169281Z","iopub.status.busy":"2024-10-17T11:54:05.168882Z","iopub.status.idle":"2024-10-17T11:54:05.174591Z","shell.execute_reply":"2024-10-17T11:54:05.173630Z","shell.execute_reply.started":"2024-10-17T11:54:05.169245Z"},"trusted":true},"outputs":[],"source":["#print(misclassified_indices.shape, X_test.shape, 1 - len(misclassified_indices)/len(X_test))"]},{"cell_type":"markdown","metadata":{},"source":["# Testing Time (Run the Import cell at the top first)\n","\n","It's essentially the same as above but we use the whole plot instead of performing a split, but I want to make the whole process clear"]},{"cell_type":"markdown","metadata":{},"source":["## Read and Preprocess the Training Dataset"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T12:40:00.398473Z","iopub.status.busy":"2024-10-17T12:40:00.397337Z","iopub.status.idle":"2024-10-17T12:40:02.022713Z","shell.execute_reply":"2024-10-17T12:40:02.021882Z","shell.execute_reply.started":"2024-10-17T12:40:00.398429Z"},"trusted":true},"outputs":[],"source":["# train_data = pd.read_csv('/kaggle/input/train-set/train.txt', sep='\\t', header=None, names=['Title', 'Origin', 'Genre', 'Director', 'Plot'])\n","\n","# def preprocess_text(text):\n","#     text = text.strip()\n","#     text = re.sub('\\s+', ' ', text)\n","#     return text\n","\n","# train_data['Clean_Plot'] = train_data['Plot'].apply(preprocess_text)\n","\n","test_data = pd.read_csv('/kaggle/input/test-set/test_no_labels.txt', \n","                        sep='\\t', header=None, names=['Title', 'Origin', 'Director', 'Plot'])\n","\n","test_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T12:40:02.214216Z","iopub.status.busy":"2024-10-17T12:40:02.213873Z","iopub.status.idle":"2024-10-17T12:40:02.225400Z","shell.execute_reply":"2024-10-17T12:40:02.224563Z","shell.execute_reply.started":"2024-10-17T12:40:02.214182Z"},"trusted":true},"outputs":[],"source":["# train_data.head()\n","\n","test_data['Clean_Plot'] = test_data['Plot'].apply(preprocess_text)\n","\n","test_encodings = tokenize_data(test_data['Clean_Plot'])"]},{"cell_type":"markdown","metadata":{},"source":["## X_train and y_train on the whole plot"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T12:40:06.987452Z","iopub.status.busy":"2024-10-17T12:40:06.987045Z","iopub.status.idle":"2024-10-17T12:40:06.996178Z","shell.execute_reply":"2024-10-17T12:40:06.995249Z","shell.execute_reply.started":"2024-10-17T12:40:06.987411Z"},"trusted":true},"outputs":[],"source":["# encoder = LabelEncoder()\n","# train_data['GenreEncoded'] = encoder.fit_transform(train_data['Genre'])\n","\n","# X_train, y_train = train_data['Clean_Plot'], train_data['GenreEncoded']"]},{"cell_type":"markdown","metadata":{},"source":["## Tokenize the training data and import model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T12:40:08.805851Z","iopub.status.busy":"2024-10-17T12:40:08.805458Z","iopub.status.idle":"2024-10-17T12:40:08.836128Z","shell.execute_reply":"2024-10-17T12:40:08.835128Z","shell.execute_reply.started":"2024-10-17T12:40:08.805813Z"},"trusted":true},"outputs":[],"source":["# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","# print(f\"Using device: {device}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T12:40:10.518153Z","iopub.status.busy":"2024-10-17T12:40:10.517282Z","iopub.status.idle":"2024-10-17T12:42:05.111260Z","shell.execute_reply":"2024-10-17T12:42:05.110351Z","shell.execute_reply.started":"2024-10-17T12:40:10.518112Z"},"trusted":true},"outputs":[],"source":["from transformers import DistilBertForSequenceClassification\n","\n","model = DistilBertForSequenceClassification.from_pretrained(\n","    'distilbert-base-uncased', \n","    num_labels=len(encoder.classes_)\n",").to(device)\n","\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","\n","def tokenize_data(texts, max_length=512):\n","    return tokenizer(\n","       texts.tolist(),\n","       padding=True,\n","       truncation=True,\n","       max_length=max_length,\n","       return_tensors='pt'\n","   )\n","\n","train_encodings = tokenize_data(X_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T12:43:40.463135Z","iopub.status.busy":"2024-10-17T12:43:40.462261Z","iopub.status.idle":"2024-10-17T12:43:40.470169Z","shell.execute_reply":"2024-10-17T12:43:40.469138Z","shell.execute_reply.started":"2024-10-17T12:43:40.463080Z"},"trusted":true},"outputs":[],"source":["X_train.shape"]},{"cell_type":"markdown","metadata":{},"source":["## Encode the dataset"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T12:42:05.113261Z","iopub.status.busy":"2024-10-17T12:42:05.112923Z","iopub.status.idle":"2024-10-17T12:42:05.121036Z","shell.execute_reply":"2024-10-17T12:42:05.120042Z","shell.execute_reply.started":"2024-10-17T12:42:05.113228Z"},"trusted":true},"outputs":[],"source":["class MovieGenreDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: val[idx] for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","    \n","def tokenize_data(texts, max_length=512):\n","    return tokenizer(\n","       texts.tolist(),\n","       padding=True,\n","       truncation=True,\n","       max_length=max_length,\n","       return_tensors='pt'\n","   )\n","\n","train_dataset = MovieGenreDataset(train_encodings, y_train)"]},{"cell_type":"markdown","metadata":{},"source":["## Training on the whole plot"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T12:43:48.147918Z","iopub.status.busy":"2024-10-17T12:43:48.146902Z","iopub.status.idle":"2024-10-17T12:54:50.251449Z","shell.execute_reply":"2024-10-17T12:54:50.250601Z","shell.execute_reply.started":"2024-10-17T12:43:48.147872Z"},"trusted":true},"outputs":[],"source":["from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n","model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(encoder.classes_)).to(device)\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","training_args = TrainingArguments(\n","    output_dir='./model_testing',\n","    eval_strategy=\"no\", # don't forget to not evaluate\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n","    logging_steps=100,\n","    logging_dir='./logs',\n","    report_to=\"none\"\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    data_collator=data_collator,\n",")\n","\n","trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["## Import the testing data, preprocess and tokenize it"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T12:54:50.253804Z","iopub.status.busy":"2024-10-17T12:54:50.253141Z","iopub.status.idle":"2024-10-17T12:54:50.334752Z","shell.execute_reply":"2024-10-17T12:54:50.333680Z","shell.execute_reply.started":"2024-10-17T12:54:50.253767Z"},"trusted":true},"outputs":[],"source":["test_data = pd.read_csv('/kaggle/input/test-set/test_no_labels.txt', \n","                        sep='\\t', header=None, names=['Title', 'Origin', 'Director', 'Plot'])\n","\n","test_data.head()"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T12:54:50.336330Z","iopub.status.busy":"2024-10-17T12:54:50.335947Z","iopub.status.idle":"2024-10-17T12:54:56.143368Z","shell.execute_reply":"2024-10-17T12:54:56.142419Z","shell.execute_reply.started":"2024-10-17T12:54:50.336290Z"},"trusted":true},"outputs":[],"source":["test_data['Clean_Plot'] = test_data['Plot'].apply(preprocess_text)\n","\n","test_encodings = tokenize_data(test_data['Clean_Plot'])"]},{"cell_type":"markdown","metadata":{},"source":["## Custom Testing Dataset"]},{"cell_type":"markdown","metadata":{},"source":["## Load the Model and Tokenizer"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T12:55:09.738934Z","iopub.status.busy":"2024-10-17T12:55:09.738540Z","iopub.status.idle":"2024-10-17T12:55:09.745433Z","shell.execute_reply":"2024-10-17T12:55:09.744419Z","shell.execute_reply.started":"2024-10-17T12:55:09.738895Z"},"trusted":true},"outputs":[],"source":["class MovieGenreTestDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings):\n","        self.encodings = encodings\n","\n","    def __getitem__(self, idx):\n","        item = {key: val[idx] for key, val in self.encodings.items()}\n","        return item\n","\n","    def __len__(self):\n","        return len(self.encodings['input_ids'])\n","\n","\n","test_dataset = MovieGenreTestDataset(test_encodings)"]},{"cell_type":"markdown","metadata":{},"source":["## Perform the predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T12:55:13.203031Z","iopub.status.busy":"2024-10-17T12:55:13.202618Z","iopub.status.idle":"2024-10-17T12:55:16.817849Z","shell.execute_reply":"2024-10-17T12:55:16.816926Z","shell.execute_reply.started":"2024-10-17T12:55:13.202966Z"},"trusted":true},"outputs":[],"source":["model.eval() # puts the model in evaluation mode, instead of training\n","\n","predictions_output = trainer.predict(test_dataset)\n","predicted_labels = np.argmax(predictions_output.predictions, axis=-1)\n","\n","predicted_genres = encoder.inverse_transform(predicted_labels)"]},{"cell_type":"markdown","metadata":{},"source":["## Writes them to a File"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T12:55:18.160022Z","iopub.status.busy":"2024-10-17T12:55:18.159600Z","iopub.status.idle":"2024-10-17T12:55:18.166191Z","shell.execute_reply":"2024-10-17T12:55:18.165205Z","shell.execute_reply.started":"2024-10-17T12:55:18.159958Z"},"trusted":true},"outputs":[],"source":["assert len(predicted_genres) == len(test_data), \"Number of predictions does not match number of plots\"\n","\n","with open('predicted_genres.txt', 'w', encoding='utf-8') as f:\n","    for genre in predicted_genres:\n","        f.write(f\"{genre}\\n\")\n","\n","# don't forget to manually remove the last \\n !!!!"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T11:38:20.720681Z","iopub.status.busy":"2024-10-17T11:38:20.720352Z","iopub.status.idle":"2024-10-17T11:38:20.726398Z","shell.execute_reply":"2024-10-17T11:38:20.725583Z","shell.execute_reply.started":"2024-10-17T11:38:20.720650Z"},"trusted":true},"outputs":[],"source":["# import os\n","# print(os.listdir('./saved_model_0.699'))"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T11:38:20.727622Z","iopub.status.busy":"2024-10-17T11:38:20.727353Z","iopub.status.idle":"2024-10-17T11:38:20.736636Z","shell.execute_reply":"2024-10-17T11:38:20.735812Z","shell.execute_reply.started":"2024-10-17T11:38:20.727592Z"},"trusted":true},"outputs":[],"source":["# from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n","\n","# # Load the saved model and tokenizer from the directory\n","# model = DistilBertForSequenceClassification.from_pretrained('./saved_model_0.699', from_tf=False)\n","# tokenizer = DistilBertTokenizer.from_pretrained('./saved_model_0.699', from_tf=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## this should be at the end"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5854405,"sourceId":9597127,"sourceType":"datasetVersion"},{"datasetId":5868062,"sourceId":9615680,"sourceType":"datasetVersion"}],"dockerImageVersionId":30786,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
