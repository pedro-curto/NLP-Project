{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pip Installs I Made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install pandas\n",
    "!pip install seaborn\n",
    "!pip install scikit-learn\n",
    "!pip install wordcloud\n",
    "!pip install transformers\n",
    "!pip install textblob\n",
    "!pip install spacy\n",
    "!pip install imblearn\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('datasets/train.txt', sep='\\t', header=None, names=['Title', 'Origin', 'Genre', 'Director', 'Plot'])\n",
    "\n",
    "print(data.head())\n",
    "\n",
    "print(f\"{data.isnull().sum()}, {len(data.isnull())}\")\n",
    "\n",
    "print(\"Genre value counts\")\n",
    "print(data['Genre'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds stop words from nltk\n",
    "#stop_words = stopwords.words('english')\n",
    "\n",
    "stop_words = [\"the\", \"to\", \"of\", \"a\", 'and', 'is', 'his', 'in', 'he', \n",
    "            'that', 'her', \"with\", \"by\", \"for\", \"him\", \"the\", \"as\", \"who\",\n",
    "            \"on\", \"she\", \"but\", \"from\", \"has\", \"they\", \"an\", \"at\", \"their\", \"are\",\n",
    "            \"into\", \"he\", \"out\", \"it\", \"up\", \"be\", \"was\", \"when\", \"not\", \"them\", \"which\",\n",
    "            \"then\", \"after\", \"about\", \"where\", \"one\", \"have\", \"When\", \"After\", \"tells\", \"him.\",\n",
    "            \"back\", \"She\", \"will\", \"while\", \"all\", \"two\", \"In\", \"had\", \"been\", \"They\",\n",
    "            \"get\", \"only\", \"also\", \"before\", \"off\", \"being\", \"As\", \"goes\", \"takes\",\n",
    "            \"this\", \"other\", \"take\", \"tries\", \"A\", \"her.\", \"go\", \"gets\", \"can\", \"man\", \"so\",\n",
    "            \"over\", \"through\", \"down\", \"help\", \"new\", \"him,\", \"now\", \"comes\", \"next\", \"himself\",\n",
    "            \"later\", \"however\", \"away\", \"there\", \"during\", \"both\", \"first\", \"again\", \"no\", \"way\", \"own\",\n",
    "            \"some\", \"another\", \"more\", \"becomes\", \"make\", \"does\", \"what\", \"begins\", \"meanwhile\", \"just\",\n",
    "            \"asks\", \"if\", \"because\", \"soon\", \"having\", \"its\", \"eventually\", \"come\", \"still\", \"between\", \"father\",\n",
    "            \"finds\", \"house\", \"home\", \"find\"# TODO check if father should be here or not\n",
    "            ]\n",
    "\n",
    "# TODO keep adding from list below \n",
    "# also, add words from the top 10 most frequent words in each genre\n",
    "'''\n",
    "SHOULD WE CONSIDER THESE:\n",
    "('finds', 3769), ('find', 3721), ('help', 2395), ('film', 2240)], ('leave', 2411), ('leaves', 2400), ('decides', 2220)\n",
    "('meets', 1932), ('arrives', 1918), ('room', 1887), ('girl', 1877), ('return', 1874), ('group', 1859), ('sees', 1857), \n",
    "('dead', 1842), ('old', 1837), ('story', 1832), ('see', 1832), ('brother', 1806), ('each', 1791), ('three', 1775), \n",
    "('body', 1760), ('falls', 1754), ('finally', 1738), ('fight', 1738), ('reveals', 1690), ('school', 1684), \n",
    "('gang', 1662), ('wants', 1655), ('head', 1618), ('local', 1612), ('attempts', 1593), \n",
    "('gives', 1588), ('against', 1583), ('work', 1581), ('same', 1565), ('discovers', 1565), ('together', 1548)]\n",
    "]'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
    "    text = text.strip(' ')\n",
    "    tokens = text.split()\n",
    "    filtered = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation + string.digits), '', text)\n",
    "    text = re.sub('\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def clean_text2(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
    "    text = text.strip(' ')\n",
    "    return text\n",
    "\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    tokens = text.split()\n",
    "    filtered = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = text.split()\n",
    "    filtered = [word for word in tokens if word not in stop_words]\n",
    "    return filtered\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = text.split()\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "# cleaning, tokenizing and removing stopwords TODO does it make sense to tokenize?\n",
    "# even if we tokenize, does it make sense to tokenize like I'm doing?\n",
    "data['Clean_Plot'] = data['Plot'].apply(preprocess_text)\n",
    "\n",
    "#data['Clean_Plot'] = data['Clean_Plot'].apply(tokenize_and_remove_stopwords)\n",
    "\n",
    "# lemmatization -- TODO does it make sense to lemmatize? I think not\n",
    "#data['Clean_Plot'] = data['Clean_Plot'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writes the Clean Plot to a file for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to txt the data['Clean_Plot']\n",
    "data['Clean_Plot'].to_csv('datasets/clean_plot_with_nonalpha.txt', index=False)\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check genre distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(y='Genre', data=data, order=data['Genre'].value_counts().index)\n",
    "plt.title('Distribution of Movie Genres')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Genre')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Cloud for Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from wordcloud import WordCloud\n",
    "#\n",
    "#def generate_wordcloud(key):\n",
    "#    genres = data['Genre'].unique()\n",
    "#    for genre in genres:\n",
    "#        text = ' '.join(data[data['Genre'] == genre][key])\n",
    "#        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "#        plt.figure(figsize=(15, 7.5))\n",
    "#        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "#        plt.title(f'Word Cloud for {genre} Movies in {key}')\n",
    "#        plt.axis('off')\n",
    "#        plt.show()\n",
    "#\n",
    "#generate_wordcloud('Plot')\n",
    "#generate_wordcloud('Clean_Plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most Frequent Words for Genre (Not Word Cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def plot_top_words_by_genre(key, num_top_words=20):\n",
    "#     genres = data['Genre'].unique()\n",
    "#     plt.figure(figsize=(12, len(genres) * 5))\n",
    "\n",
    "#     for idx, genre in enumerate(genres):\n",
    "#         genre_data = data[data['Genre'] == genre][key]\n",
    "\n",
    "#         vectorizer = CountVectorizer(stop_words='english')\n",
    "#         genre_matrix = vectorizer.fit_transform(genre_data)\n",
    "\n",
    "#         word_freq = pd.DataFrame(genre_matrix.toarray(), columns=vectorizer.get_feature_names_out()).sum(axis=0)\n",
    "        \n",
    "#         top_words = word_freq.nlargest(num_top_words)\n",
    "\n",
    "#         plt.subplot(len(genres), 1, idx + 1)\n",
    "#         plt.bar(top_words.index, top_words.values, color='skyblue')\n",
    "#         plt.title(f'Top {num_top_words} Words for {genre} Genre in {key}')\n",
    "#         plt.xlabel('Words')\n",
    "#         plt.ylabel('Frequency')\n",
    "#         plt.xticks(rotation=45)\n",
    "#         #print(top_words)\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "#plot_top_words_by_genre('Plot')\n",
    "#plot_top_words_by_genre('Clean_Plot')\n",
    "\n",
    "# from the 50 most common words per genre, I want to get, for each genre, the ones that better differentiate it from the others\n",
    "\n",
    "def get_top_words_by_genre(key, num_top_words=50):\n",
    "    genres = data['Genre'].unique()\n",
    "    top_words_by_genre = {}\n",
    "\n",
    "    for genre in genres:\n",
    "        genre_data = data[data['Genre'] == genre][key]\n",
    "\n",
    "        vectorizer = CountVectorizer(stop_words='english')\n",
    "        genre_matrix = vectorizer.fit_transform(genre_data)\n",
    "\n",
    "        word_freq = pd.DataFrame(genre_matrix.toarray(), columns=vectorizer.get_feature_names_out()).sum(axis=0)\n",
    "        \n",
    "        top_words = word_freq.nlargest(num_top_words)\n",
    "        top_words_by_genre[genre] = top_words\n",
    "\n",
    "    return top_words_by_genre\n",
    "\n",
    "# I want to visualize them\n",
    "def plot_top_words_by_genre(top_words_by_genre, num_top_words=20):\n",
    "    genres = data['Genre'].unique()\n",
    "    plt.figure(figsize=(12, len(genres) * 5))\n",
    "\n",
    "    for idx, genre in enumerate(genres):\n",
    "        top_words = top_words_by_genre[genre]\n",
    "\n",
    "        plt.subplot(len(genres), 1, idx + 1)\n",
    "        plt.bar(top_words.index, top_words.values, color='skyblue')\n",
    "        plt.title(f'Top {num_top_words} Words for {genre} Genre')\n",
    "        plt.xlabel('Words')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "top_words_by_genre = get_top_words_by_genre('Clean_Plot')\n",
    "plot_top_words_by_genre(top_words_by_genre)\n",
    "\n",
    "# add unique words as a new feature for each genre\n",
    "def add_unique_words(data, top_words_by_genre):\n",
    "    for genre in data['Genre'].unique():\n",
    "        data[f'{genre}_unique_words'] = data['Clean_Plot'].apply(lambda x: len(set(x.split()) & set(top_words_by_genre[genre].index)))\n",
    "    \n",
    "# how can I visualize this?\n",
    "add_unique_words(data, top_words_by_genre)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_length(key):\n",
    "    data['Plot_Length'] = data[key].apply(lambda x: len(x.split()))\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.boxplot(x='Genre', y='Plot_Length', data=data)\n",
    "    plt.title(f'Plot Length Distribution by Genre in {key}')\n",
    "    plt.xlabel('Genre')\n",
    "    plt.ylabel('Number of Words in Plot')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    print(data.groupby('Genre')['Plot_Length'].describe())\n",
    "\n",
    "plot_length('Clean_Plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Analysis of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def freq_analysis(key):\n",
    "    # print the top 100 most frequent words\n",
    "    all_words = ' '.join(data[key]).split()\n",
    "    word_freq = Counter(all_words)\n",
    "    common_words = word_freq.most_common(100)\n",
    "    print(common_words)\n",
    "\n",
    "    # plot the top 20 most frequent words\n",
    "    all_words = ' '.join(data[key]).split()\n",
    "    word_freq = Counter(all_words)\n",
    "    common_words = word_freq.most_common(20)\n",
    "    df_common_words = pd.DataFrame(common_words, columns=['Word', 'Frequency'])\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.barplot(x='Frequency', y='Word', data=df_common_words)\n",
    "    plt.title(f'Top 20 Most Frequent Words in {key}')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Word')\n",
    "    plt.show()\n",
    "    \n",
    "freq_analysis('Clean_Plot')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_frequency_heatmap(key):\n",
    "    \n",
    "    vectorizer = CountVectorizer(stop_words=stop_words, max_features=1000)\n",
    "    dtm = vectorizer.fit_transform(data[key])\n",
    "    dtm_df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    genre_term_freq = dtm_df.groupby(data['Genre']).mean()\n",
    "\n",
    "    top_words = dtm_df.sum().nlargest(20).index\n",
    "    genre_term_freq_top = genre_term_freq[top_words]\n",
    "\n",
    "    plt.figure(figsize=(12,8))\n",
    "    sns.heatmap(genre_term_freq_top, annot=True, fmt=\".2f\", cmap=\"YlGnBu\")\n",
    "    plt.title(f'Average Term Frequencies per Genre in {key}')\n",
    "    plt.show()\n",
    "    \n",
    "term_frequency_heatmap('Clean_Plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def get_sentiment_polarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "def get_sentiment_subjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "data['Sentiment_polarity'] = data['Plot'].apply(get_sentiment_polarity)\n",
    "data['Sentiment_subjectivity'] = data['Plot'].apply(get_sentiment_subjectivity)\n",
    "\n",
    "def plot_sentiment_by_genre(modifier):\n",
    "    # Set the size of the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create a boxplot of sentiment by genre\n",
    "    sns.boxplot(x='Genre', y=modifier, data=data)\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.title('Sentiment Distribution by Genre')\n",
    "    plt.xlabel('Genre')\n",
    "    plt.ylabel(modifier)\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels if necessary for readability\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_sentiment_by_genre('Sentiment_polarity')\n",
    "plot_sentiment_by_genre('Sentiment_subjectivity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Counts (Experiment!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy\n",
    "#\n",
    "#nlp = spacy.load('en_core_web_sm')\n",
    "#\n",
    "#def count_entities(text):\n",
    "#    doc = nlp(text)\n",
    "#    return len(doc.ents)\n",
    "#\n",
    "#data['Entity_Count'] = data['Plot'].apply(count_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging (Experiment!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def pos_counts(text):\n",
    "#    doc = nlp(text)\n",
    "#    pos_counts = doc.count_by(spacy.attrs.POS)\n",
    "#    return pos_counts\n",
    "#\n",
    "#data['POS_Counts'] = data['Plot'].apply(pos_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Extraction (Experiment!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy\n",
    "#\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "#\n",
    "## Define a function to extract proper nouns (names) from the plot\n",
    "#def extract_names(text):\n",
    "#    doc = nlp(text)\n",
    "#    return [ent.text for ent in doc.ents if ent.label_ in ['PERSON']]\n",
    "#\n",
    "## Apply the function and get unique names\n",
    "#data['Extracted_Names'] = data['Plot'].apply(lambda x: extract_names(x) if pd.notnull(x) else [])\n",
    "#unique_names = set([name for sublist in data['Extracted_Names'] for name in sublist])\n",
    "#print(f\"Number of unique names extracted from the plot: {len(unique_names)}\")\n",
    "\n",
    "# Check if these names are actors by cross-referencing with a known list of actors (if available)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Genre-Specific Keywords (Experiment!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#horror_keywords = []\n",
    "#\n",
    "#def contains_horror_keywords(text):\n",
    "#    return int(any(word in text for word in horror_keywords))\n",
    "#\n",
    "#data['Horror_Keywords'] = data['Clean_Plot'].apply(contains_horror_keywords)\n",
    "#\n",
    "#print(data['Horror_Keywords'])#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "data['Genre_Label'] = encoder.fit_transform(data['Genre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Above are shitty models that I've played with for fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction and Model Building (Multinomial Naive Bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=7000, min_df=20, stop_words=stop_words, ngram_range=(1, 3))\n",
    "X = tfidf.fit_transform(data['Clean_Plot']).toarray()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['Genre'])\n",
    "chi2_selector = SelectKBest(chi2, k=5000)\n",
    "X_kbest = chi2_selector.fit_transform(X, y)\n",
    "# we need to apply some sampling technique because the class imbalance is affecting the results\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#X_train, y_train = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "nb_classifier = MultinomialNB(alpha=0.1)\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#!pip install torch\n",
    "#!pip install tokenizers\n",
    "!pip install transformers==4.6.0\n",
    "#!pip install simpletransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, transformers, tokenizers\n",
    "torch.__version__, transformers.__version__, tokenizers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    data['Plot'], data['Genre_Label'], \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=data['Genre_Label']\n",
    ")\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_encodings = tokenizer(\n",
    "    X_train.tolist(), \n",
    "    truncation=True, \n",
    "    padding=True, \n",
    "    max_length=512\n",
    ")\n",
    "val_encodings = tokenizer(\n",
    "    X_val.tolist(), \n",
    "    truncation=True, \n",
    "    padding=True, \n",
    "    max_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MovieGenreDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels.reset_index(drop=True)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels.iloc[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', \n",
    "    num_labels=len(encoder.classes_)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory\n",
    "    num_train_epochs=3,              # Total number of training epochs\n",
    "    per_device_train_batch_size=8,   # Batch size per device during training\n",
    "    per_device_eval_batch_size=8,    # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    logging_dir='./logs',            # Directory for storing logs\n",
    "    evaluation_strategy='epoch',     # Evaluation strategy\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,     # Load best model at end of training\n",
    "    metric_for_best_model='accuracy',\n",
    "    greater_is_better=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "scores = cross_val_score(nb_classifier, X, y, cv=skf, scoring='accuracy')\n",
    "\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean accuracy:\", scores.mean())\n",
    "\n",
    "\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Misclassified Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_indices = np.where(y_test != y_pred)[0]\n",
    "print(len(misclassified_indices))\n",
    "for idx in misclassified_indices[:5]:\n",
    "    print(f\"Plot: {data.iloc[idx]['Plot']}\")\n",
    "    print(f\"Actual Genre: {label_encoder.inverse_transform([y_test[idx]])[0]}\")\n",
    "    print(f\"Predicted Genre: {label_encoder.inverse_transform([y_pred[idx]])[0]}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_classifier = SVC(kernel='linear')\n",
    "svc_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred_svc = svc_classifier.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svc))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_svc, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Info for GPT models (useful for io-preview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print info about Genre Distribution\n",
    "print(\"### Genre Distribution Analysis ###\")\n",
    "genre_counts = data['Genre'].value_counts()\n",
    "print(\"The dataset contains the following number of movies per genre:\\n\")\n",
    "for genre, count in genre_counts.items():\n",
    "    print(f\"Genre: {genre}, Count: {count}\")\n",
    "print(\"\\nThis shows the overall distribution of genres in the dataset, highlighting the most and least represented genres.\")\n",
    "\n",
    "# Print info about the Most Frequent Words per Genre in Plot and Clean_Plot\n",
    "def print_top_words_info(key, num_top_words=20):\n",
    "    print(f\"\\n### Most Frequent Words Analysis for {key} ###\")\n",
    "    genres = data['Genre'].unique()\n",
    "    for genre in genres:\n",
    "        genre_data = data[data['Genre'] == genre][key]\n",
    "        vectorizer = CountVectorizer(stop_words='english')\n",
    "        genre_matrix = vectorizer.fit_transform(genre_data)\n",
    "        word_freq = pd.DataFrame(genre_matrix.toarray(), columns=vectorizer.get_feature_names_out()).sum(axis=0)\n",
    "        top_words = word_freq.nlargest(num_top_words)\n",
    "        print(f\"\\nTop {num_top_words} words for {genre} genre in {key}:\\n{top_words}\\n\")\n",
    "    print(f\"This analysis identifies the most common words used in each genre's movie plot descriptions.\")\n",
    "\n",
    "print_top_words_info('Clean_Plot')\n",
    "\n",
    "# Print info about Plot Length Distribution per Genre in Plot and Clean_Plot\n",
    "def print_plot_length(key):\n",
    "    data['Plot_Length'] = data[key].apply(lambda x: len(x.split()))\n",
    "    print(data.groupby('Genre')['Plot_Length'].describe())\n",
    "\n",
    "print_plot_length('Clean_Plot')\n",
    "\n",
    "# Print info about the Top 20 Most Frequent Words in Plot and Clean_Plot\n",
    "def print_top_100_words_info(key):\n",
    "    print(f\"\\n### Top 100 Most Frequent Words Analysis for {key} ###\")\n",
    "    all_words = ' '.join(data[key]).split()\n",
    "    word_freq = Counter(all_words)\n",
    "    common_words = word_freq.most_common(100)\n",
    "    print(f\"The top 100 most frequent words in {key} are:\\n{common_words}\\n\")\n",
    "    print(\"This analysis highlights the most common words in the dataset, giving an idea of the typical vocabulary used in movie plot descriptions.\")\n",
    "\n",
    "print_top_100_words_info('Clean_Plot')\n",
    "\n",
    "# Print info about the Term Frequency Heatmap per Genre in Plot and Clean_Plot\n",
    "def print_term_frequency_heatmap_info(key):\n",
    "    print(f\"\\n### Term Frequency Heatmap Analysis for {key} ###\")\n",
    "    vectorizer = CountVectorizer(stop_words=stop_words, max_features=1000)\n",
    "    dtm = vectorizer.fit_transform(data[key])\n",
    "    dtm_df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    genre_term_freq = dtm_df.groupby(data['Genre']).mean()\n",
    "    top_words = dtm_df.sum().nlargest(20).index\n",
    "    print(f\"The average term frequencies for the top 20 words per genre in {key} are as follows:\\n\")\n",
    "    print(genre_term_freq[top_words])\n",
    "    print(\"The heatmap provides a visual representation of word frequency patterns across genres, which helps to understand which words are more prevalent in each genre.\")\n",
    "\n",
    "print_term_frequency_heatmap_info('Clean_Plot')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
